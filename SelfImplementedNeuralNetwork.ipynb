{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SelfImplementedNeuralNetwork\n",
    "A neural network implemented using only numpy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initiliase\n",
    "Initialises the weights and biases of the network as all zero ~~with uniform distributions from -1 to 1~~. The network comprises 4 layers having 28Â² = 784, 128, 128 and 10 neurons respectively."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def init_network():\n",
    "    # The layer sizes\n",
    "    n0 = 28*28\n",
    "    n1 = 128\n",
    "    n2 = 128\n",
    "    n3 = 10\n",
    "    \n",
    "    init_weights = lambda m, n: np.random.uniform(-1,1,(n,m))\n",
    "    init_biases = lambda m: np.random.uniform(-1,1,(m,))\n",
    "    # init_weights = lambda m, n: np.zeros((n,m)) # <- Attention: This function flips n and m for convenient input\n",
    "    #  init_biases = lambda m: np.zeros((m,))\n",
    "    \n",
    "    layer0_1_weights = init_weights(n0,n1)\n",
    "    layer0_1_biases = init_biases(n1)\n",
    "    layer0_1 = [layer0_1_weights, layer0_1_biases]\n",
    "    \n",
    "    layer1_2_weights = init_weights(n1,n2)\n",
    "    layer1_2_biases = init_biases(n2)\n",
    "    layer1_2 = [layer1_2_weights, layer1_2_biases]\n",
    "    \n",
    "    layer2_3_weights = init_weights(n2,n3)\n",
    "    layer2_3_biases = init_biases(n3)\n",
    "    layer2_3 = [layer2_3_weights, layer2_3_biases]\n",
    "    \n",
    "    return [layer0_1, layer1_2, layer2_3]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Activation functions\n",
    "The *ReLU* (Rectified Linear Unit) activation function is defined as $\\text{ReLU}(x)=\\text{max}(0,x)$ and is the go-to activation function for hidden layers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def relu(x):\n",
    "    return np.maximum(0,x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The *Sotfmax* acitvation function is a generalization of the *Sigmoid* function. It is applied to a 1D-Array of values which are squished to the interval $[0,1)$ such that the sum of all values is 1. Thereby, we get a true probability distribution which is quite convenient. It is defined as\n",
    "\n",
    "$$\\sigma(x)_i=\\frac{e^{x_i}}{\\sum_{j}e^{x_j}}\\quad\\forall\\;i.$$\n",
    "\n",
    "To be compatible with forward propagation on a 2D-Array of multiple training examples we extend the function to also take 2D-Arrays as a parameter and perform the softmax row-wise. (How this works in detail is annotated in the comments.)\n",
    "\n",
    "Moreover, if the arrays contain big values we quickly get an overflow for `np.exp`. Therefore, we subtract the maximum value of each row (or for the 1D case: simply the maximum) from all values which doesn't alter the endresult of our computation, but rids us of some possible (pseudo-)infinities along the way. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def softmax(x):\n",
    "    # If x is a 2D-array of multiple training exmaples, we transpose so that axis=0 is the axis of a single \n",
    "    # training example. (We can't simply use axis=1 because that would break if we use the function for a \n",
    "    # 1D-array, i.e. only one training example. Transposing first and then using axis=0 works in both cases.)\n",
    "    x = x.T\n",
    "    # To avoid overflow of np.exp (doesn't alter the value)\n",
    "    x = x - np.max(x, axis=0)\n",
    "    # The main calculation\n",
    "    result = np.exp(x) / np.sum(np.exp(x), axis=0)\n",
    "    # Tranpsose back to normal form where each row is a training example (only makes a difference if array is 2D;\n",
    "    # doesn't change anything if array is 1D.)\n",
    "    return result.T"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Forward propagation\n",
    "Here we implement forward propagation. This function can either take a 1D-Array of a single training example or a 2D-Array of multiple training examples where each row is a training example. The transformations (`np.array.T`) are necessary so that the dot product and the vector addition work in the 2D case. They can be ignored in the 1D case.\n",
    "\n",
    "For backpropagation we not only need the endresult, but all intermediate results. If the option `intermediate_results` is set to `True` (default is `False`) we return a tuple $(y^{(1)},o^{(1)},y^{(2)},o^{(2)},y^{(3)},o^{(3)})$. Otherwise we simply return $o^{(3)}$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def feed_forward(data, network, intermediate_results=False):\n",
    "    # You can either pass only one training example,\n",
    "    # then we add a new axis so that axis=0 is the axis\n",
    "    # of training examples.\n",
    "    if data.ndim == 1:\n",
    "        x = data[np.newaxis, :]\n",
    "    elif data.ndim == 2:\n",
    "        x = data\n",
    "    else:\n",
    "        raise Exception(\"Too many dimensions\")\n",
    "        \n",
    "    y1 = np.einsum(\"jk,ik\", network[0][0], x) + network[0][1]\n",
    "    o1 = relu(y1)\n",
    "    \n",
    "    y2 = np.einsum(\"jk,ik\", network[1][0], o1) + network[1][1]\n",
    "    o2 = relu(y2)\n",
    "    \n",
    "    y3 = np.einsum(\"jk,ik\", network[2][0], o2) + network[2][1]\n",
    "    o3 = softmax(y3)\n",
    "    \n",
    "    if not intermediate_results:\n",
    "        return o3\n",
    "    else:\n",
    "        return y1, o1, y2, o2, y3, o3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def feed_forward_old(data, network, intermediate_results=False): \n",
    "    y1s, o1s, y2s, o2s, y3s, o3s = [], [], [], [], [], []\n",
    "    \n",
    "    for datapoint in data:\n",
    "        x = datapoint\n",
    "        \n",
    "        y1 = np.dot(network[0][0], x) + network[0][1]\n",
    "        o1 = relu(y1)\n",
    "\n",
    "        y2 = np.dot(network[1][0], o1) + network[1][1]\n",
    "        o2 = relu(y2)\n",
    "\n",
    "        y3 = np.dot(network[2][0], o2) + network[2][1]\n",
    "        o3 = softmax(y3)\n",
    "        \n",
    "        y1s.append(y1)\n",
    "        y2s.append(y2)\n",
    "        y3s.append(y3)\n",
    "        o1s.append(o1)\n",
    "        o2s.append(o2)\n",
    "        o3s.append(o3)\n",
    "        \n",
    "    if not intermediate_results:\n",
    "        return np.array(o3s)\n",
    "    else:\n",
    "        return (np.array(y1s), \n",
    "               np.array(o1s), \n",
    "               np.array(y2s), \n",
    "               np.array(o2s),\n",
    "               np.array(y3s), \n",
    "               np.array(o3s))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get data\n",
    "We use the mnist handwritten digit classification data which is shipped with keras."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "network = init_network()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "(x_train_raw, y_train), (x_test_raw, y_test) = tf.keras.datasets.mnist.load_data()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Flatten the data to make it a well-behaved input for our neural network:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train = x_train_raw.reshape(60000, 28*28)\n",
    "x_test = x_test_raw.reshape(10000, 28*28)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Comparing old and new implementation of feed forward"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 10.1 s\n"
     ]
    }
   ],
   "source": [
    "%time a1, a2, a3, a4, a5, a6 = feed_forward(x_train, network, intermediate_results=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 19.1 s\n"
     ]
    }
   ],
   "source": [
    "%time b1, b2, b3, b4, b5, b6 = feed_forward_old(x_train, network, intermediate_results=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 8.22 s\n"
     ]
    }
   ],
   "source": [
    "%time a = feed_forward(x_train, network)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 7.31 s\n"
     ]
    }
   ],
   "source": [
    "%time b = feed_forward_old(x_train, network)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "New implementation in NumPy seems to be a little better than the python-loop implementation, but the difference is smaller than I expected. Let's check whether the `einsum`s are really correct:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-14802.54636527,  31744.0743766 , -71116.22668884,  -6157.43616531,\n",
       "       -28363.8420773 ,  41135.58574794, -43756.55572725,  42298.65365992,\n",
       "        26275.49106977,  36896.10255654])"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a5[5000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-14802.54636527,  31744.0743766 , -71116.22668884,  -6157.43616531,\n",
       "       -28363.8420773 ,  41135.58574794, -43756.55572725,  42298.65365992,\n",
       "        26275.49106977,  36896.10255654])"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "b5[5000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.allclose(a5, b5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.allclose(a6,b6)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We also check whether the softmax function works correctly:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "a = np.array([softmax(elem) for elem in b5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "b = softmax(b5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.allclose(a,b)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The feed forward function can now either take a single example or an array of examples!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cost function\n",
    "First, the loss function, i.e. the error for a single training example. There are a multitude of loss functions, though there is an \"industry-standard\" for each type of problem. While the simpler and better-known MSE (*mean squared error*) is used for regression problems, one uses *cross entropy loss* for categorization problems (which fits our case precisely).\n",
    "\n",
    "Given a prediction $\\hat{y_i}$ and the correct solution $y_i$ (in our case for $0\\leq i<10$), we can define *cross entropy loss* as \n",
    "$$H(y,\\hat{y})=-\\sum_{i=0}^9 y_i\\cdot \\text{log}(\\hat{y}_i).$$\n",
    "Since we know that $y_i$ is zero for all but one $i$ (for which it has value 1), we can simplify this to\n",
    "$$H(y,\\hat{y})=-\\text{log}(\\hat{y}_k)$$\n",
    "where $k$ is the index of the correct solution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cross_entropy_loss(y_hat, y):\n",
    "    y_val = y_hat[y]\n",
    "    # To avoid division by zero\n",
    "    if y_val == 0:\n",
    "        # #The numpy epsilon (np.finfo(float).eps) is apparently not really \n",
    "        # # the smallest number possible; by trying out I found the limit for \n",
    "        # # np.log to not throw an error was at about eps*10^(-307).\n",
    "        # y_val = np.finfo(\"float64\").eps*10**-307\n",
    "        # Update: \n",
    "        # np.spacing(0) gives the smallest number bigger than 0\n",
    "        y_val = np.spacing(0)\n",
    "    return -np.log(y_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "result = feed_forward(x_train, network)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "744.4400719213812"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cross_entropy_loss(result[10], y_train[10])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we define the cost function, i.e. the error for a whole training set (or subset). In other words\n",
    "$$E(Y,\\hat{Y})=\\sum_{\\forall\\;y,\\,\\hat{y}}H(y,\\hat{y}).$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cross_entropy_cost(y_hats, ys):\n",
    "    return np.sum(cross_entropy_loss(y_hat, y) for y_hat, y in zip(y_hats, ys))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "40847995.16900215"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cross_entropy_cost(result, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Backpropagation\n",
    "Now, finally the training will be implemented.\n",
    "\n",
    "### Definitions\n",
    "\n",
    "- Let the layers be labeled as $n=0,\\dots,3$.\n",
    "- Let $x_i$ be the $i$-th $x$-value, that is the $i$-th input of the network ($0\\leq i<28^2$).\n",
    "- Let $y_i^{(n)}$ be the values of the $i$-th neuron of layer $n$ *before* the activation function ($0\\leq i<128$ for $n\\in\\{1,2\\}$, $0\\leq i<10$ for $n=3$).\n",
    "- Let $o_i^{(n)}$ be the values of the $i$-th neuron of layer $n$ *after* the activation function.\n",
    "    - $o_i^{(n)}=\\text{ReLU}(y_i^{n})$ for $n\\in\\{1,2\\}$\n",
    "    - $o_i^{(3)}=\\sigma(y^{(3)})_i$\n",
    "- Let $w_{ij}^{(n)}$ represent the weight on the connection of the $i$-th neuron in layer n to the $j$-th neuron in layer n+1. (Can be found in `network[n][0][j][i]`.)  \n",
    "- Let $b_i^{(n)}$ represent the bias of the $i$-th neuron in layer n+1. (Can be found in `network[n][1][i]`.)\n",
    "- Within the context of the cost/loss function we use the following definitions:\n",
    "    - $x$ and $y$ are the input and expected output of a single training example;\n",
    "    - $X$ and $Y$ are the set (or a subset) of training examples; \n",
    "    - $\\hat{y}$ is the actual output of our network, in other words: $\\hat{y}=o^{(3)}$; \n",
    "    - $\\hat{Y}$ is the set of actual outputs for a set of inputs $X$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Stochastic Gradient Descent\n",
    "\n",
    "We split the computation of the gradient into six parts:\n",
    "- Gradient of the layer 2 to 3 weights ($w^{(2)}$)\n",
    "- Gradient of the layer 3 biases ($b^{(2)}$)\n",
    "- Gradient of the layer 1 to 2 weights ($w^{(1)}$)\n",
    "- Gradient of the layer 2 biases ($b^{(1)}$)\n",
    "- Gradient of the layer 0 to 1 weights ($w^{(0)}$)\n",
    "- Gradient of the layer 1 biases ($b^{(0)}$)\n",
    "\n",
    "#### Layer 2 to 3 weights\n",
    "\n",
    "##### Overview\n",
    "Our goal is to determine\n",
    "$$\\frac{\\partial}{\\partial w_{ij}^{(2)}} \\;E(Y,\\hat{Y})$$\n",
    "$$=\\quad\\sum_{\\forall y,\\hat{y}} \\frac{\\partial}{\\partial w_{ij}^{(2)}} \\;H(y,\\hat{y})$$\n",
    "$$=\\quad\\sum_{\\forall y,\\hat{y}} \\frac{\\partial}{\\partial w_{ij}^{(2)}} \\;\\left(-\\text{log}(\\hat{y}_k)\\right),$$\n",
    "where $k$ is the correct solution for a given sample."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Cost function derivative with respect to a single weight\n",
    "Hence, let us consider\n",
    "$$\\frac{\\partial}{\\partial w_{ij}^{(2)}} \\;\\left(-\\text{log}(\\hat{y}_k)\\right)$$\n",
    "$$=\\quad \\frac{\\partial}{\\partial w_{ij}^{(2)}} \\;\\left(-\\text{log}(o^{(3)}_k)\\right)$$\n",
    "$$=\\quad \\frac{\\partial}{\\partial o^{(3)}_k}\\;\\left(-\\text{log}(o^{(3)}_k)\\right)\\cdot\\frac{\\partial o^{(3)}_k}{\\partial w_{ij}^{(2)}}$$\n",
    "$$=\\quad -\\frac{1}{o^{(3)}_k}\\cdot \\frac{\\partial}{\\partial w_{ij}^{(2)}} \\;\\sigma(y^{(3)})_k$$\n",
    "$$=\\quad -\\frac{1}{o^{(3)}_k}\\cdot \\frac{\\partial}{\\partial w_{ij}^{(2)}} \\;\\frac{e^{y^{(3)}_k}}{\\sum_{\\forall r}e^{y^{(3)}_r}}.$$\n",
    "$$=\\quad -\\frac{1}{o^{(3)}_k}\\cdot \\left(\\sum_{\\forall s}\\frac{\\partial}{\\partial y^{(3)}_s} \\;\\frac{e^{y^{(3)}_k}}{\\sum_{\\forall r}e^{y^{(3)}_r}} \\cdot \\frac{\\partial y^{(3)}_s}{\\partial w_{ij}^{(2)}} \\right).$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To simplify it more, let us focus on\n",
    "$$\\frac{\\partial}{\\partial y^{(3)}_s} \\;\\frac{e^{y^{(3)}_k}}{\\sum_{\\forall r}e^{y^{(3)}_r}}$$\n",
    "$$=\\quad \\frac{\\partial}{\\partial e^{y^{(3)}_s}} \\;\\frac{e^{y^{(3)}_k}}{\\sum_{\\forall r}e^{y^{(3)}_r}} \\cdot \\frac{\\partial e^{y^{(3)}_s}}{\\partial y^{(3)}_s}$$\n",
    "$$=\\quad e^{y^{(3)}_s}\\cdot\\frac{\\partial}{\\partial e^{y^{(3)}_s}} \\;\\frac{e^{y^{(3)}_k}}{\\sum_{\\forall r}e^{y^{(3)}_r}}$$\n",
    "$$=\\quad e^{y^{(3)}_s} \\cdot \n",
    "\\frac{ \n",
    "    \\frac{\\partial}{ \\partial e^{y^{(3)}_s} } \\; e^{y^{(3)}_k} \\cdot \\left(\\sum_{\\forall r}e^{y^{(3)}_r}\\right) \n",
    "    -  e^{y^{(3)}_k} \\cdot \\frac{\\partial}{ \\partial e^{y^{(3)}_s} } \\; \\left(\\sum_{\\forall r}e^{y^{(3)}_r}\\right)\n",
    "}{\n",
    "    \\left(\\sum_{\\forall r}e^{y^{(3)}_r}\\right)^2\n",
    "}.$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here, we need to make a distinction between $s=k$ and $s\\neq k$. If $s=k$, we get\n",
    "$$e^{y^{(3)}_s} \\cdot\n",
    "\\frac{ \n",
    "    \\sum_{\\forall r}e^{y^{(3)}_r} \n",
    "    -  e^{y^{(3)}_k}\n",
    "}{\n",
    "    \\left(\\sum_{\\forall r}e^{y^{(3)}_r}\\right)^2\n",
    "}$$\n",
    "$$=\\quad \n",
    "\\frac{\n",
    "    e^{y^{(3)}_s}\n",
    "}{\n",
    "    \\sum_{\\forall r}e^{y^{(3)}_r}\n",
    "}\n",
    "\\cdot\n",
    "\\frac{\n",
    "    \\sum_{\\forall r}e^{y^{(3)}_r} \n",
    "    -  e^{y^{(3)}_k}\n",
    "}{\n",
    "    \\sum_{\\forall r}e^{y^{(3)}_r}\n",
    "}\n",
    "$$\n",
    "$$=\\quad \\sigma(y^{(3)})_s \\cdot \\left( 1 - \\sigma(y^{(3)})_k \\right)$$\n",
    "$$=\\quad o^{(3)}_s \\cdot \\left( 1 - o^{(3)}_k \\right).$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If $s\\neq k$, we get\n",
    "$$e^{y^{(3)}_s} \\cdot\n",
    "\\frac{ \n",
    "    -  e^{y^{(3)}_k}\n",
    "}{\n",
    "    \\left(\\sum_{\\forall r}e^{y^{(3)}_r}\\right)^2\n",
    "}$$\n",
    "$$=\\quad - \\sigma(y^{(3)})_s \\cdot \\sigma(y^{(3)})_k$$\n",
    "$$=\\quad - o^{(3)}_s \\cdot o^{(3)}_k.$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Furthermore, we need to evaluate\n",
    "$$\\frac{\\partial y^{(3)}_s}{\\partial w_{ij}^{(2)}}.$$\n",
    "We know that\n",
    "$$y^{(3)}_s = \\sum_{\\forall t} w_{ts}^{(2)}o^{(2)}_t + b_s^{(3)}.$$\n",
    "If $j=s$, we have\n",
    "$$\\frac{\\partial y^{(3)}_s}{\\partial w_{ij}^{(2)}} = o^{(2)}_i.$$\n",
    "Else, if $j\\neq s$, we have\n",
    "$$\\frac{\\partial y^{(3)}_s}{\\partial w_{ij}^{(2)}} = 0.$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The equation we considered before was\n",
    "$$-\\frac{1}{o^{(3)}_k}\\cdot \\left(\\sum_{\\forall s}\\frac{\\partial}{\\partial y^{(3)}_s} \\;\\frac{e^{y^{(3)}_k}}{\\sum_{\\forall r}e^{y^{(3)}_r}} \\cdot \\frac{\\partial y^{(3)}_s}{\\partial w_{ij}^{(2)}} \\right),$$\n",
    "where -- as we see from our last result -- all terms but one turn out to be zero. What remains is\n",
    "$$-\\frac{1}{o^{(3)}_k}\\cdot \\left(\\frac{\\partial}{\\partial y^{(3)}_j} \\;\\frac{e^{y^{(3)}_k}}{\\sum_{\\forall r}e^{y^{(3)}_r}}\\cdot o_i^{(2)}\\right).$$\n",
    "Now, we have to distinguish between $j=k$ and $j\\neq k$, which yields us for $j=k$\n",
    "$$\\frac{\\partial}{\\partial w_{ij}^{(2)}} \\;H(y,\\hat{y}) = \n",
    "    -\\frac{1}{o^{(3)}_k}\n",
    "    \\cdot \n",
    "    o^{(3)}_j \\cdot \\left( 1 - o^{(3)}_k \\right)\n",
    "    \\cdot \n",
    "    o_i^{(2)},$$\n",
    "and for $j\\neq k$\n",
    "$$\\frac{\\partial}{\\partial w_{ij}^{(2)}} \\;H(y,\\hat{y}) = \n",
    "    \\frac{1}{o^{(3)}_k}\n",
    "    \\cdot\n",
    "    o^{(3)}_j \\cdot o^{(3)}_k\n",
    "    \\cdot \n",
    "    o_i^{(2)}$$\n",
    "$$=\\quad o^{(3)}_j\\cdot o_i^{(2)}.$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Unifying it into a single matrix\n",
    "Our weights matrix $W^{(2)}$ has dimensions $n^{(3)} \\times n^{(2)}$. (This means the indices $i$ and $j$ in $w_{ij}^{(2)}$ are actually the wrong way around.) Let us call the gradient of $H(y,\\hat y)$ w.r.t. the weight matrix \n",
    "$$\\nabla_{w^{(2)}} \\,H(y,\\hat y).$$ \n",
    "This gradient shall be a $n^{(3)} \\times n^{(2)}$ matrix as well. (Hence, contrary to popular usage, we extend the notion of gradient from simply being a vector to also being a matrix; this is useful because we also treat the weights as matrices which simplifies both updating the weights according to the gradient and computing the gradient non-iteratively easier.)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There is a distinction to be made between $j=k$ and $j\\neq k$, i.e. the $k$-th row in the gradient matrix needs a special treating. Luckily, considering the case $j=k$ as derived above we see\n",
    "$$\\frac{\\partial}{\\partial w_{ij}^{(2)}} \\;H(y,\\hat{y}) = \n",
    "    -\\frac{1}{o^{(3)}_k}\n",
    "    \\cdot \n",
    "    o^{(3)}_j \\cdot \\left( 1 - o^{(3)}_k \\right)\n",
    "    \\cdot \n",
    "    o_i^{(2)}$$\n",
    "$$= \\quad\n",
    "    -\\frac{1}{o^{(3)}_k} \\cdot o^{(3)}_j \\cdot o_i^{(2)}\n",
    "    \\; + \\;\n",
    "    \\frac{1}{o^{(3)}_k} \\cdot o^{(3)}_j \\cdot o^{(3)}_k \\cdot o_i^{(2)}$$\n",
    "$$= \\quad\n",
    "    - o_i^{(2)}\n",
    "    \\; + \\;\n",
    "    o^{(3)}_j \\cdot o_i^{(2)},$$\n",
    "where the second summand coincides with the case $j\\neq k$. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Thus, we are able to split the unification of the many partial derivatives into a single gradient matrix into two steps\n",
    "1. Compute the matrix where the element in the $i$-th column and $j$-th row is $o^{(3)}_j \\cdot o_i^{(2)}$.\n",
    "2. Subtract from the $k$-th row a (row) vector where the $i$-th column is $o_i^{(2)}$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The computation in step 1 is\n",
    "$$\\quad o^{(3)} \\otimes o^{(2)}$$\n",
    "where $a\\otimes b = a\\cdot b^T$ denotes the *outer product*, *matrix product* or *tensor product* of two vectors (called `np.outer`). In the second step, we simply subtract $o^{(2)}$ (as a row vector) from the $k$-th column of the result of step 1. (Expressing this operation mathematically is not necessary, here we just have to consider implementing it efficiently.) \n",
    "\n",
    "Lastly, we can sum this matrix for multiple training examples\n",
    "$$\\nabla_{W^{(2)}} \\;E(Y,\\hat{Y}) = \\sum_{\\forall\\;y,\\hat y} \\nabla_{W^{(2)}} \\;H(y,\\hat y).$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Layer 3 biases\n",
    "Here, we examine\n",
    "$$\\frac{\\partial}{\\partial b_{j}^{(2)}} \\;E(Y,\\hat{Y})$$\n",
    "$$=\\quad\\sum_{\\forall y,\\hat{y}} \\frac{\\partial}{\\partial b_{j}^{(2)}} \\;H(y,\\hat{y}),$$\n",
    "which can be derived in a very analogous way. The key difference arises when determining\n",
    "$$\\frac{\\partial y^{(3)}_s}{\\partial b_{j}^{(2)}}.$$\n",
    "If $j=s$, we have\n",
    "$$\\frac{\\partial y^{(3)}_s}{\\partial b_{j}^{(2)}} = 1,$$\n",
    "instead of $o_i^{(2)}$ which we would have got for for $w_{ij}^{(2)}$. Else, if $j\\neq s$, we still have\n",
    "$$\\frac{\\partial y^{(3)}_s}{\\partial b_{j}^{(2)}} = 0.$$\n",
    "Replacing every occasion of $o_i^{(2)}$ with $1$, we see that $\\nabla_{b^{(2)}} \\;H(y,\\hat y)$ is a $n^{(3)}$-dimensional vector where the $j$-th row is $o_j^{(3)}$ if $j\\neq k$ and $o_j^{(3)}-1$ if $j=k$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def stochastic_gradient_descent(x_train, y_train, network, batch_size=1000, epochs=3):\n",
    "    num_examples = len(x_train)\n",
    "    \n",
    "    iterations_per_epoch = num_examples//batch_size\n",
    "    \n",
    "    for _ in range(epochs):\n",
    "        for i in range(iterations_per_epoch):\n",
    "            X = x_train[i:i+batch_size]\n",
    "            Y = y_train[i:i+batch_size]\n",
    "\n",
    "            y1, o1, y2, o2, y3, o3 = feed_forward(X, network, intermediate_results=True)\n",
    "\n",
    "            print(cross_entropy_cost(o3,Y))\n",
    "\n",
    "            ## Layer 2to3 weights\n",
    "            # Step 1\n",
    "            w2_grad = np.einsum(\"ij,ik\",o3, o2)\n",
    "            # Step 2\n",
    "            for i, y in enumerate(Y):\n",
    "                w2_grad[y] -= o2[i]\n",
    "            # Update weights\n",
    "            network[2][0] -= w2_grad*0.0001\n",
    "\n",
    "            ## Layer 3 biases\n",
    "            # Step 1\n",
    "            b3_grad\n",
    "            # Step 2\n",
    "    \n",
    "    # Layer 1to2 weights\n",
    "    \n",
    "    # Layer 2 biases\n",
    "    \n",
    "    # Layer 0to1 weights\n",
    "    \n",
    "    # Layer 1 biases"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "o3 = feed_forward(x_train[0], network)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "network = init_network()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "669883.8712313533\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'b3_grad' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-29-5d1bd1b6fc78>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mstochastic_gradient_descent\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnetwork\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m<ipython-input-26-27509e7f198f>\u001b[0m in \u001b[0;36mstochastic_gradient_descent\u001b[1;34m(x_train, y_train, network, batch_size, epochs)\u001b[0m\n\u001b[0;32m     24\u001b[0m             \u001b[1;31m## Layer 3 biases\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     25\u001b[0m             \u001b[1;31m# Step 1\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 26\u001b[1;33m             \u001b[0mb3_grad\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     27\u001b[0m             \u001b[1;31m# Step 2\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     28\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'b3_grad' is not defined"
     ]
    }
   ],
   "source": [
    "stochastic_gradient_descent(x_train, y_train, network)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "5//3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y1, o1, y2, o2, y3, o3 = feed_forward(x_train[0:50], network, intermediate_results=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "o2.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(o3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.einsum(\"ij,ik\",o3,o2).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "network[2][0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y1, o1, y2, o2, y3, o3 = feed_forward(x_train, network, intermediate_results=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = np.einsum(\"ij,ik\",o3[0:1],o2[0:1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "b = np.outer(o3[0],o2[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.allclose(a,b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
