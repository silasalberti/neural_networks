{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SelfImplementedNeuralNetwork\n",
    "A neural network implemented using only numpy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initiliase\n",
    "Initialises the weights and biases of the network with uniform distributions from -1 to 1. The network comprises 4 layers having 784, 128, 128 and 10 neurons respectively."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [],
   "source": [
    "def init_network():\n",
    "    # The layer sizes\n",
    "    n0 = 28*28\n",
    "    n1 = 128\n",
    "    n2 = 128\n",
    "    n3 = 10\n",
    "    \n",
    "    init_weights = lambda m, n: np.random.uniform(-1,1,(n,m))\n",
    "    init_biases = lambda m: np.random.uniform(-1,1,(m,))\n",
    "    \n",
    "    layer0_1_weights = init_weights(n0,n1)\n",
    "    layer0_1_biases = init_biases(n1)\n",
    "    layer0_1 = layer0_1_weights, layer0_1_biases\n",
    "    \n",
    "    layer1_2_weights = init_weights(n1,n2)\n",
    "    layer1_2_biases = init_biases(n2)\n",
    "    layer1_2 = layer1_2_weights, layer1_2_biases\n",
    "    \n",
    "    layer2_3_weights = init_weights(n2,n3)\n",
    "    layer2_3_biases = init_biases(n3)\n",
    "    layer2_3 = layer2_3_weights, layer2_3_biases\n",
    "    \n",
    "    return layer0_1, layer1_2, layer2_3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Activation functions\n",
    "*ReLU* is defined as $\\text{max}(0,x)$ and is the go-to activation function for hidden layers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [],
   "source": [
    "def relu(x):\n",
    "    return np.maximum(0,x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The *Sotfmax* acitvation function is a generalization of the *Sigmoid* function. It is applied to a 1D-Array of values which are squished to the interval $[0,1)$ such that the sum of all values is 1. Thereby, we get a true probability distribution which is quite convenient. It is defined as\n",
    "\n",
    "$$\\frac{e^{x_i}}{\\sum_{j}e^{x_j}}.$$\n",
    "\n",
    "To be compatible with forward propagation on a 2D-Array of multiple training examples we extend the function to also take 2D-Arrays as a parameter and perform the softmax row-wise. (How this works in detail is annotated in the comments.)\n",
    "\n",
    "Moreover, if the arrays contain big values we quickly get an overflow for `np.exp`. Therefore, we subtract the maximum value of each row (or for the 1D case: simply the maximum) from all values which doesn't alter the endresult of our computation, but rids us of some possible (pseudo-)infinities along the way. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 386,
   "metadata": {},
   "outputs": [],
   "source": [
    "def softmax(x):\n",
    "    # If x is a 2D-array of multiple training exmaples, we transpose so that axis=0 is the axis of a single \n",
    "    # training example. (We can't simply use axis=1 because that would break if we use the function for a \n",
    "    # 1D-array, i.e. only one training example. Transposing first and then using axis=0 works in both cases.)\n",
    "    x = x.T\n",
    "    # To avoid overflow of np.exp (doesn't alter the value)\n",
    "    x = x - np.max(x, axis=0)\n",
    "    # The main calculation\n",
    "    result = np.exp(x) / np.sum(np.exp(x), axis=0)\n",
    "    # Tranpsose back to normal form where each row is a training example (only makes a difference if array is 2D;\n",
    "    # doesn't change anything if array is 1D.)\n",
    "    return result.T"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Forward propagation\n",
    "Here we implement forward propagation. This function can either take a 1D-Array of a single training example or a 2D-Array of multiple training examples where each row is a training example. The transformations (`np.array.T`) are necessary so that the dot product and the vector addition work in the 2D case. They can be ignored in the 1D case."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 281,
   "metadata": {},
   "outputs": [],
   "source": [
    "def feed_forward(data, network):\n",
    "    layer0 = data\n",
    "\n",
    "#   # How it would work iteratively:\n",
    "#   layer1 = []\n",
    "#   for weights, bias in zip(network[0][0], network[0][1]):\n",
    "#      neuron = relu(np.dot(weights, layer0) + bias)\n",
    "#      layer1.append(neuron)  \n",
    "#   layer1 = np.array(layer1)\n",
    "    \n",
    "    layer1 = relu(np.dot(network[0][0], layer0.T).T + network[0][1])\n",
    "        \n",
    "    layer2 = relu(np.dot(network[1][0], layer1.T).T + network[1][1])\n",
    "            \n",
    "    layer3 = softmax(np.dot(network[2][0], layer2.T).T + network[2][1])\n",
    "    \n",
    "    return layer3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get data\n",
    "We use the mnist handwritten digit classification data which is shipped with keras."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 277,
   "metadata": {},
   "outputs": [],
   "source": [
    "network = init_network()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 278,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "(x_train_raw, y_train), (x_test_raw, y_test) = tf.keras.datasets.mnist.load_data()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Flatten the data to make it a well-behaved input for our neural network:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 279,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train = x_train_raw.reshape(60000, 28*28)\n",
    "x_test = x_test_raw.reshape(10000, 28*28)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The feed forward function can either take a single example or an array of examples:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 285,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "        True])"
      ]
     },
     "execution_count": 285,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "feed_forward(x_train[10], network) == feed_forward(x_train, network)[10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cost function\n",
    "First, the loss function, i.e. the error for a single training example. There are a multitude of loss functions, though there is an \"industry-standard\" for each type of problem. While the simpler and better-known MSE (*mean squared error*) is used for regression problems, one uses *cross entropy loss* for categorization problems (which fits our case precisely).\n",
    "\n",
    "Given a prediction $\\hat{y}_i$ and the correct solution $y_i$ (in our case for $0\\leq i<10$), we can define *cross entropy loss* as \n",
    "$$H(y,\\hat{y})=-\\sum_{i=0}^9 y_i\\cdot \\text{log}(\\hat{y}_i).$$\n",
    "Since we know that $y_i$ is zero for all but one $i$, we can simplify this to\n",
    "$$H(y,\\hat{y})=-\\text{log}(\\hat{y}_k)$$\n",
    "where $k$ is the index of the correct solution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 636,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cross_entropy_loss(y_hat, y):\n",
    "    y_val = y_hat[y]\n",
    "    # To avoid division by zero\n",
    "    if y_val == 0:\n",
    "        # #The numpy epsilon (np.finfo(float).eps) is apparently not really \n",
    "        # # the smallest number possible; by trying out I found the limit for \n",
    "        # # np.log to not throw an error was at about eps*10^(-307).\n",
    "        # y_val = np.finfo(\"float64\").eps*10**-307\n",
    "        # Update: \n",
    "        # np.spacing(0) gives the smallest number bigger than 0\n",
    "        y_val = np.spacing(0)\n",
    "    return -np.log(y_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 605,
   "metadata": {},
   "outputs": [],
   "source": [
    "result = feed_forward(x_train, network)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 606,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "744.4400719213812"
      ]
     },
     "execution_count": 606,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cross_entropy_loss(result[10], y_train[10])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then, the cost function, i.e. the error for a whole training set (or subset)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 607,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cross_entropy_cost(y_hats, ys):\n",
    "    return np.sum(cross_entropy_loss(y_hat, y) for y_hat, y in zip(y_hats, ys))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 608,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "40916654.30157135"
      ]
     },
     "execution_count": 608,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cross_entropy_cost(result, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Backpropagation\n",
    "Now, finally the training will be implemented.\n",
    "\n",
    "### Definitions\n",
    "\n",
    "- Let the layers be labeled from $n=0$ to $n=3$.\n",
    "- Let $x_i^{(0)}$ be the $i$-th $x$-value, that is the $i$-th input of the network ($0\\leq i<28^2$).\n",
    "- Let $x_i^{(1)}$ and $x_i^{(2)}$ be the value of the $i$-th neuron of (hidden) layer 1 or 2, respectively ($0\\leq i<128$).\n",
    "- Let $y_i$ be the $i$-th output, that is the value of the $i$-th neuron in layer 3 ($0\\leq i< 10$).\n",
    "- Let $w_{ij}^{(n)}$ represent the weight on the connection of the $i$-th neuron in layer n to the $j$-th neuron in layer n+1. (Can be found in `network[n][0][j][i]`.)  \n",
    "\n",
    "### Stochastic Gradient Descent\n",
    "Let us consider the gradient aka partial derivatives for specific groups of parameters:\n",
    "#### Layer 2 to 3 weights\n",
    "We consider the weights \n",
    "$$w_{ij}^{(2)} \\qquad\\text{where}\\; 0\\leq i\\leq127,\\; 0\\leq j\\leq9$$ \n",
    "which represents the weight on the connection of the $i$-th neuron in layer 2 to the $j$-th neuron in layer 3. (Can be found in `network[2][0][j][i]`.)\n",
    "\n",
    "Our goal is to determine\n",
    "$$\\frac{\\partial}{\\partial w_{ij}^{(2)}} \\;\\text{cross_entropy_cost}$$\n",
    "$$=\\quad\\sum_{\\forall\\;\\text{samples}} \\frac{\\partial}{\\partial w_{ij}^{(2)}} \\;\\text{cross_entropy_loss}$$\n",
    "$$=\\quad\\sum_{\\forall\\;\\text{samples}} \\frac{\\partial}{\\partial w_{ij}^{(2)}} \\;\\left(-\\text{log}(y_k)\\right),$$\n",
    "where $k$ is the correct solution for a given sample.\n",
    "\n",
    "Hence, let us consider\n",
    "$$\\frac{\\partial}{\\partial w_{ij}^{(2)}} \\;\\left(-\\text{log}(y_k)\\right)$$\n",
    "$$=\\quad \\frac{\\partial}{\\partial y_k}\\;\\left(-\\text{log}(y_k)\\right)\\cdot\\frac{\\partial y_k}{\\partial w_{ij}^{(2)}}$$\n",
    "$$=\\quad -\\frac{1}{y_k}\\cdot \\frac{\\partial}{\\partial w_{ij}^{(2)}} \\sum_{r=0}^{127} w_{rk}^{(2)}\\cdot x_r^{(2)}$$\n",
    "$$=\\quad -\\frac{1}{y_k}\\cdot \\frac{\\partial}{\\partial w_{ij}^{(2)}} w_{ik}^{(2)}\\cdot x_r^{(2)}.$$\n",
    "We see that if $j\\neq k$ \n",
    "$$ \\frac{\\partial}{\\partial w_{ij}^{(2)}} \\;\\text{cross_entropy_loss} = 0.$$\n",
    "Otherwise if $j = k$\n",
    "$$\\frac{\\partial}{\\partial w_{ij}^{(2)}} \\;\\text{cross_entropy_loss}$$\n",
    "$$=\\quad -\\frac{1}{y_k}\\cdot \\frac{\\partial}{\\partial w_{ik}^{(2)}} w_{ik}^{(2)}\\cdot x_r^{(2)}.$$\n",
    "$$=\\quad -\\frac{1}{y_k}\\cdot x_r^{(2)}.$$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 620,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sgd(x_train, y_train, network, batch_size=50, epochs=3):\n",
    "    x_single = x_train[0]\n",
    "    y_single = y_train[0]\n",
    "    \n",
    "    result = feed_forward(x_single, network)\n",
    "    \n",
    "    print(result.shape)\n",
    "    \n",
    "    # Layer 2to3 weights\n",
    "    \n",
    "    # Layer 3 biases\n",
    "    \n",
    "    # Layer 1to2 weights\n",
    "    \n",
    "    # Layer 2 biases\n",
    "    \n",
    "    # Layer 0to1 weights\n",
    "    \n",
    "    # Layer 1 biases"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
